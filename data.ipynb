{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c6d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PandasAI configured with LiteLLM + Ollama!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandasai as pai\n",
    "from pandasai_litellm.litellm import LiteLLM\n",
    "\n",
    "# Configure LiteLLM to use Ollama\n",
    "llm = LiteLLM(\n",
    "    model=\"ollama/mistral:7b\",\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Configure PandasAI\n",
    "pai.config.set({\n",
    "    \"llm\": llm,\n",
    "    \"verbose\": True,\n",
    "    \"enforce_privacy\": False\n",
    "})\n",
    "\n",
    "print(\"‚úÖ PandasAI configured with LiteLLM + Ollama!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5596587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column names:\n",
      "['SL NO', 'DOCTOR_NAME', 'HOSPITAL_NUMBER', 'PATIENT_NAME', 'CONTACTNO', 'DEATH/ ALIVE (7 AUG 2024)', 'DEATH/ ALIVE (OUTSIDE RX DETAILS)', 'status', 'FOLLOW UP STATUS', 'SURGERY DATE', 'Study end time', 'Calculation Time', 'time_in_days', 'time_in_mos', 'TUMOR SIZE', 'SEX', 'AGE', 'LOCATION', 'LOCATION.1', 'LATARALITY', 'HPR DETAILS', 'HPR', 'GRADE', 'PRESS_DIAGNOSIS', 'SURGERY NAME', 'SURGERY TYPE', 'GFAP', 'IDH1/ IDH2', 'ATRX', 'MGMT', 'Mib Lebeling Index', 'RT DOSE', 'RT MODALITY', 'CCRT', 'RT COMPLETION DATE', 'ADJUVANT ChT', 'START DATE', 'END DATE', 'RECC STATUS', 'OS']\n",
      "\n",
      "Cleaned column names:\n",
      "['SL_NO', 'DOCTOR_NAME', 'HOSPITAL_NUMBER', 'PATIENT_NAME', 'CONTACTNO', 'DEATH_ALIVE_7_AUG_2024', 'DEATH_ALIVE_OUTSIDE_RX_DETAILS', 'status', 'FOLLOW_UP_STATUS', 'SURGERY_DATE', 'Study_end_time', 'Calculation_Time', 'time_in_days', 'time_in_mos', 'TUMOR_SIZE', 'SEX', 'AGE', 'LOCATION', 'LOCATION1', 'LATARALITY', 'HPR_DETAILS', 'HPR', 'GRADE', 'PRESS_DIAGNOSIS', 'SURGERY_NAME', 'SURGERY_TYPE', 'GFAP', 'IDH1_IDH2', 'ATRX', 'MGMT', 'Mib_Lebeling_Index', 'RT_DOSE', 'RT_MODALITY', 'CCRT', 'RT_COMPLETION_DATE', 'ADJUVANT_ChT', 'START_DATE', 'END_DATE', 'RECC_STATUS', 'OS']\n",
      "\n",
      "‚úÖ PandasAI DataFrame created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandasai as pai\n",
    "df = pd.read_excel(r\"D:\\taipy\\GBM_V1.11.xlsx\")\n",
    "# Cell 3: Clean column names for PandasAI\n",
    "print(\"Original column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Clean column names - remove special characters and spaces\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True)\n",
    "\n",
    "print(\"\\nCleaned column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Now create the PandasAI DataFrame\n",
    "df1 = pai.DataFrame(df)\n",
    "print(\"\\n‚úÖ PandasAI DataFrame created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b56c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MCP is available!\n"
     ]
    }
   ],
   "source": [
    "# Check if MCP server packages are available\n",
    "try:\n",
    "    import mcp\n",
    "    print(\"‚úÖ MCP is available!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå MCP not installed\")\n",
    "    print(\"To install: pip install mcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97330c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing llama3.2:3b with optimized settings...\n",
      "‚úÖ llama3.2:3b: 8.28 seconds\n",
      "ÔøΩÔøΩ Response: Hello! How can I assist you today?...\n",
      "\n",
      "‚úÖ Best model: llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "# Since mcp-server-ollama doesn't exist, let's use Ollama directly with better configuration\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def test_ollama_with_better_config():\n",
    "    \"\"\"Test Ollama with optimized configuration\"\"\"\n",
    "    \n",
    "    # Test different models with optimized settings\n",
    "    models_to_test = [\n",
    "        {\n",
    "            \"name\": \"llama3.2:3b\",\n",
    "            \"payload\": {\n",
    "                \"model\": \"llama3.2:3b\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"num_ctx\": 512,  # Smaller context\n",
    "                    \"num_thread\": 4,  # Limit threads\n",
    "                    \"temperature\": 0.1  # Lower temperature for faster response\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"mistral:7b\", \n",
    "            \"payload\": {\n",
    "                \"model\": \"mistral:7b\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"num_ctx\": 512,\n",
    "                    \"num_thread\": 4,\n",
    "                    \"temperature\": 0.1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for model_test in models_to_test:\n",
    "        print(f\"\\nüîç Testing {model_test['name']} with optimized settings...\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/chat\",\n",
    "                json=model_test[\"payload\"],\n",
    "                timeout=30\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                content = data.get(\"message\", {}).get(\"content\", \"No response\")\n",
    "                print(f\"‚úÖ {model_test['name']}: {end_time - start_time:.2f} seconds\")\n",
    "                print(f\"ÔøΩÔøΩ Response: {content[:100]}...\")\n",
    "                return model_test['name']  # Return the working model\n",
    "            else:\n",
    "                print(f\"‚ùå {model_test['name']}: HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_test['name']}: Error - {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test with optimized settings\n",
    "best_model = test_ollama_with_better_config()\n",
    "if best_model:\n",
    "    print(f\"\\n‚úÖ Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No models working with optimized settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11261732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PandasAI configured with llama3.2:3b!\n",
      "üîç Testing medical data analysis...\n"
     ]
    }
   ],
   "source": [
    "# Use the best model with optimized PandasAI configuration\n",
    "if best_model:\n",
    "    import pandas as pd\n",
    "    import pandasai as pai\n",
    "    from pandasai_litellm.litellm import LiteLLM\n",
    "    \n",
    "    # Configure with the best model\n",
    "    llm = LiteLLM(\n",
    "        model=f\"ollama/{best_model}\",\n",
    "        api_base=\"http://localhost:11434\"\n",
    "    )\n",
    "    \n",
    "    # Minimal PandasAI config to avoid memory issues\n",
    "    pai.config.set({\n",
    "        \"llm\": llm,\n",
    "        \"verbose\": False,\n",
    "        \"enforce_privacy\": False,\n",
    "        \"max_retries\": 1,  # Reduce retries\n",
    "        \"enable_cache\": False  # Disable cache\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ PandasAI configured with {best_model}!\")\n",
    "    \n",
    "    # Test with simple query\n",
    "    try:\n",
    "        print(\"üîç Testing medical data analysis...\")\n",
    "        response = df1.chat(\"What is the median age of the patients?\")\n",
    "        print(f\"üìä Response: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
